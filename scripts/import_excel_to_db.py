#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Excel æ•°æ®å¯¼å…¥è„šæœ¬
å°† Excel æ ¼å¼çš„åŸºé‡‘æ—¥é¢‘æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“

ä½¿ç”¨ç¤ºä¾‹ï¼š
æµ‹è¯•æ–‡ä»¶åœ¨fund_analysis_project\data\test_fund_data.xlsx
python scripts/import_excel_to_db.py --input è·¯å¾„ --dry-run
"""

import sys
import os
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import sqlite3
import argparse
import logging
from typing import Dict, List, Tuple, Optional, Any
import io

# ä¿è¯è„šæœ¬ç‹¬ç«‹è¿è¡Œæ—¶èƒ½æ‰¾åˆ°é¡¹ç›®å†…æ¨¡å—
PROJECT_ROOT = Path(__file__).resolve().parent.parent
SRC_DIR = PROJECT_ROOT / "src"
for candidate in (PROJECT_ROOT, SRC_DIR):
    if str(candidate) not in sys.path:
        sys.path.insert(0, str(candidate))

from src.utils.runtime_env import add_project_paths

# ç»Ÿä¸€å¤„ç†å†»ç»“/æ™®é€šç¯å¢ƒä¸‹çš„è·¯å¾„ä¸å¯¼å…¥
REPO_ROOT, STORAGE_ROOT = add_project_paths()

from src.utils.database import fund_db
from src.utils.fund_code_manager import fund_code_manager
from src.utils.output_manager import get_output_manager
import config

# æ—¥å¿—é…ç½®ï¼Œç¨åæ ¹æ®è¾“å‡ºç›®å½•ç»‘å®šæ–‡ä»¶
logger = logging.getLogger(__name__)


def configure_logging(log_path: Path, verbose: bool = False):
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    log_path.parent.mkdir(parents=True, exist_ok=True)
    safe_stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

    logging.basicConfig(
        level=logging.DEBUG if verbose else logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(safe_stdout),
            logging.FileHandler(str(log_path), encoding='utf-8')
        ],
        force=True
    )


class ExcelImporter:
    """Excel æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, db_path: str = None):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        self.db_path = Path(db_path or config.DATABASE_PATH)
        self.connection = None
        self.cursor = None
        
        # åˆ—åæ˜ å°„ï¼ˆæ”¯æŒå¤šç§å¯èƒ½çš„å¤§å°å†™å’Œåç§°ï¼‰
        self.column_mappings = {
            'fund_id': ['fund_id', 'fund_code', 'åŸºé‡‘ä»£ç ', 'ä»£ç '],
            'date': ['date', 'äº¤æ˜“æ—¥æœŸ', 'æ—¥æœŸ', 'datetime'],
            'nav': ['nav', 'unit_nav', 'å•ä½å‡€å€¼', 'å‡€å€¼'],
            'cumulative_nav': ['cumulative_nav', 'ç´¯è®¡å‡€å€¼', 'ç´¯è®¡nav', 'cum_nav'],
            'daily_growth': ['daily_growth', 'æ—¥å¢é•¿ç‡', 'å¢é•¿ç‡', 'return'],
            'net_assets': ['net_assets', 'èµ„äº§å‡€å€¼', 'è§„æ¨¡', 'assets']
        }
    
    def connect(self):
        """è¿æ¥åˆ°æ•°æ®åº“"""
        self.connection = sqlite3.connect(str(self.db_path))
        self.cursor = self.connection.cursor()
    
    def disconnect(self):
        """æ–­å¼€æ•°æ®åº“è¿æ¥"""
        if self.connection:
            self.connection.close()
    
    def detect_column_names(self, df: pd.DataFrame) -> Dict[str, str]:
        """
        æ£€æµ‹ DataFrame ä¸­çš„åˆ—åï¼Œæ˜ å°„åˆ°æ ‡å‡†åˆ—å
        
        Args:
            df: è¾“å…¥çš„ DataFrame
            
        Returns:
            æ˜ å°„å­—å…¸ {æ ‡å‡†åˆ—å: å®é™…åˆ—å}
        """
        mapping = {}
        
        # è·å– DataFrame çš„å®é™…åˆ—åï¼ˆå°å†™å¤„ç†ï¼‰
        actual_columns = {col.lower().strip(): col for col in df.columns}
        
        for std_name, possible_names in self.column_mappings.items():
            for possible in possible_names:
                possible_lower = possible.lower()
                if possible_lower in actual_columns:
                    mapping[std_name] = actual_columns[possible_lower]
                    break
        
        return mapping
    
    def validate_excel_file(self, file_path: Path) -> Tuple[bool, str]:
        """
        éªŒè¯ Excel æ–‡ä»¶
        
        Args:
            file_path: Excel æ–‡ä»¶è·¯å¾„
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        if not file_path.exists():
            return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
        
        if file_path.suffix.lower() not in ['.xlsx', '.xls']:
            return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}"
        
        try:
            # å°è¯•è¯»å– Excel æ–‡ä»¶
            excel_file = pd.ExcelFile(file_path)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ sheet
            if len(excel_file.sheet_names) == 0:
                return False, "Excel æ–‡ä»¶ä¸­æ²¡æœ‰ Sheet"
            
            return True, "æ–‡ä»¶éªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, f"è¯»å– Excel æ–‡ä»¶å¤±è´¥: {str(e)}"
    
    def read_excel_data(self, file_path: Path, sheet_name: str = None) -> Optional[pd.DataFrame]:
        """
        è¯»å– Excel æ•°æ®
        
        Args:
            file_path: Excel æ–‡ä»¶è·¯å¾„
            sheet_name: Sheet åç§°ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ª Sheet
            
        Returns:
            åŒ…å«æ•°æ®çš„ DataFrameï¼Œå¤±è´¥è¿”å› None
        """
        try:
            excel_file = pd.ExcelFile(file_path)
            
            # ç¡®å®šè¦è¯»å–çš„ Sheet
            if sheet_name:
                if sheet_name not in excel_file.sheet_names:
                    logger.warning(f"Sheet '{sheet_name}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª Sheet")
                    sheet_name = excel_file.sheet_names[0]
            else:
                # å°è¯•æŸ¥æ‰¾ daily_nav Sheetï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ª Sheet
                if 'daily_nav' in excel_file.sheet_names:
                    sheet_name = 'daily_nav'
                else:
                    sheet_name = excel_file.sheet_names[0]
                    logger.info(f"ä½¿ç”¨ Sheet: {sheet_name}")
            
            # è¯»å–æ•°æ®
            df = pd.read_excel(excel_file, sheet_name=sheet_name)
            
            if df.empty:
                logger.error(f"Sheet '{sheet_name}' ä¸­æ²¡æœ‰æ•°æ®")
                return None
            
            logger.info(f"è¯»å–æˆåŠŸ: {file_path.name} - Sheet: {sheet_name}")
            logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}, åˆ—: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"è¯»å– Excel æ•°æ®å¤±è´¥: {e}")
            return None
    
    def preprocess_data(self, df: pd.DataFrame, mapping: Dict[str, str]) -> Tuple[pd.DataFrame, List[str]]:
        """
        é¢„å¤„ç†æ•°æ®ï¼šé‡å‘½ååˆ—ã€éªŒè¯æ•°æ®ã€è½¬æ¢ç±»å‹
        
        Args:
            df: åŸå§‹ DataFrame
            mapping: åˆ—åæ˜ å°„
            
        Returns:
            (å¤„ç†åçš„ DataFrame, é”™è¯¯ä¿¡æ¯åˆ—è¡¨)
        """
        errors = []
        processed_df = df.copy()
        
        # 1. é‡å‘½ååˆ—
        rename_dict = {actual: std for std, actual in mapping.items()}
        processed_df.rename(columns=rename_dict, inplace=True)
        
        # 2. æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = ['fund_id', 'date']
        missing_columns = [col for col in required_columns if col not in processed_df.columns]
        if missing_columns:
            errors.append(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")
            return processed_df, errors
        
        # 3. æ£€æŸ¥è‡³å°‘æœ‰ä¸€ä¸ªå‡€å€¼åˆ—
        nav_columns = ['nav', 'cumulative_nav']
        if not any(col in processed_df.columns for col in nav_columns):
            errors.append("å¿…é¡»è‡³å°‘æä¾› nav æˆ– cumulative_nav åˆ—ä¹‹ä¸€")
            return processed_df, errors
        
        # 4. å¤„ç†åŸºé‡‘ä»£ç æ ¼å¼
        if 'fund_id' in processed_df.columns:
            # æ ‡å‡†åŒ–åŸºé‡‘ä»£ç æ ¼å¼ï¼ˆç»Ÿä¸€ä¸ºæ•°æ®åº“æ ¼å¼ï¼‰
            processed_df['fund_id'] = processed_df['fund_id'].astype(str).apply(
                lambda x: fund_code_manager.to_database_format(x)
            )
        
        # 5. å¤„ç†æ—¥æœŸæ ¼å¼
        if 'date' in processed_df.columns:
            try:
                processed_df['date'] = pd.to_datetime(processed_df['date'], errors='coerce')
                # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆæ—¥æœŸ
                invalid_dates = processed_df['date'].isna().sum()
                if invalid_dates > 0:
                    errors.append(f"å‘ç° {invalid_dates} ä¸ªæ— æ•ˆæ—¥æœŸ")
            except Exception as e:
                errors.append(f"æ—¥æœŸå¤„ç†å¤±è´¥: {e}")
        
        # 6. å¤„ç†å‡€å€¼æ•°æ®
        for col in ['nav', 'cumulative_nav', 'daily_growth', 'net_assets']:
            if col in processed_df.columns:
                processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')
        
        # 7. å¤„ç† nav å’Œ cumulative_nav çš„è½¬æ¢
        self._handle_nav_conversion(processed_df, errors)
        
        # 8. æ’åºå’Œå»é‡
        processed_df = processed_df.sort_values(['fund_id', 'date'])
        
        return processed_df, errors
    
    def _handle_nav_conversion(self, df: pd.DataFrame, errors: List[str]):
        """
        å¤„ç† NAV å’Œ Cumulative NAV ä¹‹é—´çš„è½¬æ¢
        
        Args:
            df: å¤„ç†ä¸­çš„ DataFrame
            errors: é”™è¯¯åˆ—è¡¨
        """
        try:
            # å¦‚æœåªæœ‰ cumulative_nav æ²¡æœ‰ navï¼Œå°è¯•è½¬æ¢
            if 'cumulative_nav' in df.columns and 'nav' not in df.columns:
                logger.info("åªæœ‰ cumulative_nav æ•°æ®ï¼Œå°è¯•è½¬æ¢ä¸º nav")
                
                # ç®€å•èµ·è§ï¼Œå‡è®¾äºŒè€…ç›¸åŒï¼ˆå¯¹äºæ™®é€šåŸºé‡‘ï¼‰
                df['nav'] = df['cumulative_nav']
                
            # å¦‚æœåªæœ‰ nav æ²¡æœ‰ cumulative_navï¼Œå°è¯•è½¬æ¢
            elif 'nav' in df.columns and 'cumulative_nav' not in df.columns:
                logger.info("åªæœ‰ nav æ•°æ®ï¼Œå°è¯•è½¬æ¢ä¸º cumulative_nav")
                
                # å¯¹äºæ¯ä¸ªåŸºé‡‘ï¼Œè®¡ç®—ç´¯è®¡å‡€å€¼ï¼ˆå‡è®¾åˆ†çº¢å†æŠ•èµ„ï¼‰
                # æ³¨æ„ï¼šè¿™åªæ˜¯è¿‘ä¼¼è®¡ç®—ï¼Œå®é™…ç´¯è®¡å‡€å€¼éœ€è¦è€ƒè™‘åˆ†çº¢
                df['cumulative_nav'] = np.nan
                
                # æŒ‰åŸºé‡‘åˆ†ç»„å¤„ç†
                for fund_id, group in df.groupby('fund_id'):
                    # æŒ‰æ—¥æœŸæ’åº
                    group_sorted = group.sort_values('date')
                    
                    # å¦‚æœç¬¬ä¸€ä¸ªå‡€å€¼å¤§äº1ï¼Œå¯èƒ½æ˜¯ç´¯è®¡å‡€å€¼
                    if group_sorted['nav'].iloc[0] > 1.5:  # é˜ˆå€¼å¯è°ƒæ•´
                        df.loc[group_sorted.index, 'cumulative_nav'] = group_sorted['nav']
                    else:
                        # å¦åˆ™å‡è®¾æ˜¯å•ä½å‡€å€¼ï¼Œæ— æ³•å‡†ç¡®è®¡ç®—ç´¯è®¡å‡€å€¼
                        df.loc[group_sorted.index, 'cumulative_nav'] = group_sorted['nav']
                        errors.append(f"åŸºé‡‘ {fund_id}: æ— æ³•å‡†ç¡®è®¡ç®—ç´¯è®¡å‡€å€¼ï¼Œä½¿ç”¨å•ä½å‡€å€¼æ›¿ä»£")
                        
            logger.info("å‡€å€¼è½¬æ¢å¤„ç†å®Œæˆ")
            
        except Exception as e:
            errors.append(f"å‡€å€¼è½¬æ¢å¤±è´¥: {e}")
    
    def check_duplicates(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ£€æŸ¥é‡å¤æ•°æ®
        
        Args:
            df: å¤„ç†åçš„ DataFrame
            
        Returns:
            é‡å¤æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total_rows': len(df),
            'duplicate_count': 0,
            'unique_count': 0,
            'duplicate_examples': []
        }
        
        if df.empty:
            return stats
        
        # æ£€æŸ¥é‡å¤è¡Œï¼ˆåŸºäº fund_id å’Œ dateï¼‰
        duplicate_mask = df.duplicated(subset=['fund_id', 'date'], keep=False)
        duplicate_rows = df[duplicate_mask]
        
        stats['duplicate_count'] = len(duplicate_rows)
        stats['unique_count'] = stats['total_rows'] - stats['duplicate_count']
        
        # æ”¶é›†é‡å¤ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰
        if not duplicate_rows.empty:
            duplicate_groups = duplicate_rows.groupby(['fund_id', 'date']).head(2)
            for _, row in duplicate_groups.head(5).iterrows():
                stats['duplicate_examples'].append({
                    'fund_id': row['fund_id'],
                    'date': row['date'].strftime('%Y-%m-%d') if hasattr(row['date'], 'strftime') else row['date'],
                    'nav': row.get('nav'),
                    'cumulative_nav': row.get('cumulative_nav')
                })
        
        return stats
    
    def import_to_database(self, df: pd.DataFrame, mode: str = 'newer-wins', 
                          dry_run: bool = False) -> Dict[str, Any]:
        """
        å°†æ•°æ®å¯¼å…¥æ•°æ®åº“
        
        Args:
            df: å¤„ç†åçš„ DataFrame
            mode: é‡å¤å¤„ç†æ¨¡å¼ ('newer-wins', 'skip', 'replace')
            dry_run: è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥ï¼‰
            
        Returns:
            å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total_rows': len(df),
            'successful': 0,
            'skipped': 0,
            'failed': 0,
            'errors': [],
            'funds_affected': set(),
            'date_range': {}
        }
        
        if df.empty:
            stats['errors'].append("æ²¡æœ‰æ•°æ®å¯å¯¼å…¥")
            return stats
        
        try:
            self.connect()
            
            # æŒ‰åŸºé‡‘åˆ†ç»„å¤„ç†
            for fund_id, group in df.groupby('fund_id'):
                logger.info(f"å¤„ç†åŸºé‡‘: {fund_id} ({len(group)} è¡Œ)")
                
                # æŒ‰æ—¥æœŸæ’åº
                group_sorted = group.sort_values('date')
                
                # è®°å½•æ—¥æœŸèŒƒå›´
                if fund_id not in stats['date_range']:
                    stats['date_range'][fund_id] = {
                        'start': group_sorted['date'].min().strftime('%Y-%m-%d'),
                        'end': group_sorted['date'].max().strftime('%Y-%m-%d'),
                        'days': len(group_sorted)
                    }
                
                stats['funds_affected'].add(fund_id)
                
                # é€è¡Œå¯¼å…¥
                for idx, row in group_sorted.iterrows():
                    try:
                        result = self._import_single_row(row, mode, dry_run)
                        
                        if result == 'success':
                            stats['successful'] += 1
                        elif result == 'skipped':
                            stats['skipped'] += 1
                        elif result == 'failed':
                            stats['failed'] += 1
                            
                    except Exception as e:
                        stats['failed'] += 1
                        stats['errors'].append(f"ç¬¬ {idx} è¡Œå¯¼å…¥å¤±è´¥: {e}")
            
            if not dry_run:
                self.connection.commit()
                logger.info("æ•°æ®å·²æäº¤åˆ°æ•°æ®åº“")
            
            return stats
            
        except Exception as e:
            stats['errors'].append(f"æ•°æ®åº“å¯¼å…¥å¤±è´¥: {e}")
            return stats
            
        finally:
            self.disconnect()
    
    def _import_single_row(self, row: pd.Series, mode: str, dry_run: bool) -> str:
        """
        å¯¼å…¥å•è¡Œæ•°æ®
        
        Args:
            row: æ•°æ®è¡Œ
            mode: é‡å¤å¤„ç†æ¨¡å¼
            dry_run: è¯•è¿è¡Œæ¨¡å¼
            
        Returns:
            ç»“æœçŠ¶æ€ ('success', 'skipped', 'failed')
        """
        try:
            # å‡†å¤‡æ•°æ®
            fund_id = row['fund_id']
            date_str = row['date'].strftime('%Y-%m-%d') if hasattr(row['date'], 'strftime') else str(row['date'])
            
            nav = row.get('nav')
            cumulative_nav = row.get('cumulative_nav')
            daily_growth = row.get('daily_growth')
            
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            self.cursor.execute(
                "SELECT 1 FROM fund_daily_data WHERE fund_id = ? AND date = ?",
                (fund_id, date_str)
            )
            exists = self.cursor.fetchone() is not None
            
            # æ ¹æ®æ¨¡å¼å†³å®šæ“ä½œ
            if exists:
                if mode == 'skip':
                    logger.debug(f"è·³è¿‡é‡å¤æ•°æ®: {fund_id} - {date_str}")
                    return 'skipped'
                elif mode in ['newer-wins', 'replace']:
                    # æ›´æ–°ç°æœ‰æ•°æ®
                    if not dry_run:
                        self.cursor.execute(
                            """
                            UPDATE fund_daily_data 
                            SET nav = ?, cumulative_nav = ?, daily_growth = ?
                            WHERE fund_id = ? AND date = ?
                            """,
                            (nav, cumulative_nav, daily_growth, fund_id, date_str)
                        )
                    logger.debug(f"æ›´æ–°æ•°æ®: {fund_id} - {date_str}")
                    return 'success'
            else:
                # æ’å…¥æ–°æ•°æ®
                if not dry_run:
                    self.cursor.execute(
                        """
                        INSERT INTO fund_daily_data 
                        (fund_id, date, nav, cumulative_nav, daily_growth)
                        VALUES (?, ?, ?, ?, ?)
                        """,
                        (fund_id, date_str, nav, cumulative_nav, daily_growth)
                    )
                logger.debug(f"æ’å…¥æ–°æ•°æ®: {fund_id} - {date_str}")
                return 'success'
                
        except Exception as e:
            logger.error(f"å¯¼å…¥å•è¡Œæ•°æ®å¤±è´¥: {e}")
            return 'failed'
    
    def generate_report(self, stats: Dict[str, Any], import_mode: str, 
                       dry_run: bool = False) -> str:
        """
        ç”Ÿæˆå¯¼å…¥æŠ¥å‘Š
        
        Args:
            stats: å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯
            import_mode: å¯¼å…¥æ¨¡å¼
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œ
            
        Returns:
            æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = []
        report.append("=" * 60)
        report.append("Excel æ•°æ®å¯¼å…¥æŠ¥å‘Š")
        report.append("=" * 60)
        
        if dry_run:
            report.append("ğŸ“ è¯•è¿è¡Œæ¨¡å¼ï¼ˆæœªå®é™…å†™å…¥æ•°æ®åº“ï¼‰")
        
        report.append(f"ğŸ“Š å¯¼å…¥ç»Ÿè®¡")
        report.append(f"  æ€»è¡Œæ•°: {stats['total_rows']}")
        report.append(f"  æˆåŠŸå¯¼å…¥: {stats['successful']}")
        report.append(f"  è·³è¿‡: {stats['skipped']}")
        report.append(f"  å¤±è´¥: {stats['failed']}")
        
        report.append(f"\nğŸ¯ å¤„ç†æ¨¡å¼: {import_mode}")
        if import_mode == 'newer-wins':
            report.append("  ï¼ˆé‡å¤æ•°æ®å°†è¢«æ–°æ•°æ®è¦†ç›–ï¼‰")
        elif import_mode == 'skip':
            report.append("  ï¼ˆé‡å¤æ•°æ®å°†è¢«è·³è¿‡ï¼‰")
        elif import_mode == 'replace':
            report.append("  ï¼ˆé‡å¤æ•°æ®å°†è¢«æ›¿æ¢ï¼‰")
        
        if stats['funds_affected']:
            report.append(f"\nğŸ“ˆ å—å½±å“åŸºé‡‘ ({len(stats['funds_affected'])} åª):")
            for fund_id in sorted(stats['funds_affected']):
                date_info = stats['date_range'].get(fund_id, {})
                start = date_info.get('start', 'N/A')
                end = date_info.get('end', 'N/A')
                days = date_info.get('days', 0)
                report.append(f"  - {fund_id}: {start} åˆ° {end} ({days} å¤©)")
        
        if stats.get('duplicate_stats'):
            dup_stats = stats['duplicate_stats']
            report.append(f"\nğŸ”„ é‡å¤æ•°æ®æ£€æŸ¥:")
            report.append(f"  æ€»è¡Œæ•°: {dup_stats['total_rows']}")
            report.append(f"  é‡å¤è¡Œæ•°: {dup_stats['duplicate_count']}")
            report.append(f"  å”¯ä¸€è¡Œæ•°: {dup_stats['unique_count']}")
            
            if dup_stats['duplicate_examples']:
                report.append(f"  é‡å¤ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
                for example in dup_stats['duplicate_examples'][:3]:
                    report.append(f"    - {example['fund_id']} | {example['date']} | NAV: {example.get('nav')}")
        
        if stats['errors']:
            report.append(f"\nâŒ é”™è¯¯ä¿¡æ¯ ({len(stats['errors'])} ä¸ª):")
            for i, error in enumerate(stats['errors'][:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                report.append(f"  {i}. {error}")
            if len(stats['errors']) > 5:
                report.append(f"  ... è¿˜æœ‰ {len(stats['errors']) - 5} ä¸ªé”™è¯¯")
        
        report.append(f"\nâ° ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 60)
        
        return "\n".join(report)
    
    def save_report(self, report_content: str, output_path: Optional[Path] = None, output_manager=None):
        """
        ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            report_content: æŠ¥å‘Šå†…å®¹
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            if output_manager:
                output_path = output_manager.get_path('reports', 'data_import_report.md')
            else:
                output_path = Path("reports") / f"import_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        output_path.parent.mkdir(exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Excel æ•°æ®å¯¼å…¥å·¥å…·')
    
    parser.add_argument('--input', '-i', required=True,
                       help='Excel æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--sheet', '-s',
                       help='Sheet åç§°ï¼ˆé»˜è®¤ï¼šdaily_nav æˆ–ç¬¬ä¸€ä¸ª Sheetï¼‰')
    
    parser.add_argument('--mode', '-m', choices=['newer-wins', 'skip', 'replace'],
                       default='newer-wins',
                       help='é‡å¤æ•°æ®å¤„ç†æ¨¡å¼ï¼ˆé»˜è®¤ï¼šnewer-winsï¼‰')
    
    parser.add_argument('--dry-run', '-d', action='store_true',
                       help='è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…å†™å…¥æ•°æ®åº“ï¼‰')
    
    parser.add_argument('--output', '-o',
                       help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--db-path',
                       default=str(config.DATABASE_PATH),
                       help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šconfig.DATABASE_PATHï¼‰')

    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    start_time = datetime.now()
    output_manager = get_output_manager('import_excel_to_db', base_dir=config.REPORTS_DIR, use_timestamp=True)
    configure_logging(output_manager.get_path('logs', 'import_excel_to_db.log'), args.verbose)
    
    # åˆå§‹åŒ–å¯¼å…¥å™¨
    importer = ExcelImporter(db_path=args.db_path)
    
    # éªŒè¯æ–‡ä»¶
    file_path = Path(args.input)
    is_valid, message = importer.validate_excel_file(file_path)
    if not is_valid:
        logger.error(f"æ–‡ä»¶éªŒè¯å¤±è´¥: {message}")
        sys.exit(1)
    
    logger.info(f"å¼€å§‹å¯¼å…¥: {file_path.name}")
    logger.info(f"æ¨¡å¼: {args.mode}, è¯•è¿è¡Œ: {args.dry_run}")
    
    # è¯»å– Excel æ•°æ®
    df = importer.read_excel_data(file_path, args.sheet)
    if df is None:
        logger.error("è¯»å– Excel æ•°æ®å¤±è´¥")
        sys.exit(1)
    
    # æ£€æµ‹åˆ—å
    column_mapping = importer.detect_column_names(df)
    logger.info(f"æ£€æµ‹åˆ°åˆ—åæ˜ å°„: {column_mapping}")
    
    # é¢„å¤„ç†æ•°æ®
    processed_df, errors = importer.preprocess_data(df, column_mapping)
    
    if errors:
        logger.warning(f"æ•°æ®é¢„å¤„ç†å‘ç°é—®é¢˜: {errors}")
    
    # æ£€æŸ¥é‡å¤æ•°æ®
    duplicate_stats = importer.check_duplicates(processed_df)
    
    # å¯¼å…¥åˆ°æ•°æ®åº“
    import_stats = importer.import_to_database(
        processed_df, 
        mode=args.mode,
        dry_run=args.dry_run
    )
    
    # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
    import_stats['duplicate_stats'] = duplicate_stats
    
    # ç”ŸæˆæŠ¥å‘Š
    report = importer.generate_report(
        import_stats, 
        import_mode=args.mode,
        dry_run=args.dry_run
    )
    
    # è¾“å‡ºæŠ¥å‘Š
    print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = None

    importer.save_report(report, output_path, output_manager=output_manager)
    print(f"ğŸ“ å¯¼å…¥æŠ¥å‘Š: {output_path if output_path else output_manager.get_path('reports', 'data_import_report.md')}")

    # è¾“å‡ºç›®å½•æ‘˜è¦
    output_manager.print_summary()
    
    # æ€»ç»“
    if import_stats['failed'] == 0:
        logger.info("âœ… å¯¼å…¥å®Œæˆï¼")
    else:
        logger.warning(f"âš ï¸ å¯¼å…¥å®Œæˆï¼Œä½†æœ‰ {import_stats['failed']} ä¸ªå¤±è´¥")
    
    return 0 if import_stats['failed'] == 0 else 1


if __name__ == "__main__":
    sys.exit(main())